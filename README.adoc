:toc:
:icons: font
:source-highlighter: prettify
:project_id: homelab
:tabsize: 2

== 1. Hardware architecture

Due to the lack of free physical space at home, a software-defined IT infrastructure approach like hyper-converged infrastructure (HCI) was found ideal. HCI allows virtualizing most traditional hardware equipment using software-defined computing, networking and storage.

Enterprise-level HCI deployments doesn't require specialized hardware appliances for storage - like NAS appliances - that are very common in traditional IT deployments. Both computing and storage infrastructure can be virtualized by software using hypervisors. Since Proxmox Virtual Environment (VE) is the most common open-source Type 1 hypervisor available, it was selected as the base technology of the HCI server.

image::src/img/physical_architecture.png[]

=== Hyper-converged infrastructure (HCI) specs

The most significant hardware specs of the HCI server are:

[source]
----
>> root@proxmox:~$ lscpu
Architecture:            x86_64
  CPU op-mode(s):        32-bit, 64-bit
  Address sizes:         39 bits physical, 48 bits virtual
  Byte Order:            Little Endian
CPU(s):                  4
  On-line CPU(s) list:   0-3
Vendor ID:               GenuineIntel
  Model name:            Intel(R) Celeron(R) CPU J3455 @ 1.50GHz
    CPU family:          6
    Model:               92
    Thread(s) per core:  1
    Core(s) per socket:  4
    Socket(s):           1
----

[source]
----
>> root@proxmox:~$ more /proc/meminfo
MemTotal:        7982160 kB
----

[source]
----
>> root@proxmox:~$ smartctl -a /dev/sda
...
Device Model:     AirDisk 128GB SSD
Rotation Rate:    Solid State Device
SATA Version is:  SATA 3.2, 6.0 Gb/s (current: 6.0 Gb/s)
...

>> root@proxmox:~$ smartctl -a /dev/sdb
...
=== START OF INFORMATION SECTION ===
Model Family:     Crucial/Micron Client SSDs
Device Model:     CT480BX500SSD1
Serial Number:    2246E686DF8F
LU WWN Device Id: 5 00a075 1e686df8f
Firmware Version: M6CR056
User Capacity:    480,103,981,056 bytes [480 GB]
Sector Size:      512 bytes logical/physical
Rotation Rate:    Solid State Device
Form Factor:      2.5 inches
TRIM Command:     Available
Device is:        In smartctl database [for details use: -P show]
ATA Version is:   ACS-3 T13/2161-D revision 4
SATA Version is:  SATA 3.3, 6.0 Gb/s (current: 6.0 Gb/s)
Local Time is:    Mon Feb 13 13:04:43 2023 CET
SMART support is: Available - device has SMART capability.
SMART support is: Enabled
...
----

For my use case, these hardware specs are perfectly ok regarding cpu and memory. However, only 2 small SDD drives for both OS and data long term storage is clearly not enough to meet my storage requirements in both capacity and fault-tolerance.

**I decided to buy 2 external USB hard disk** (no more SATA ports available in such small enclosure). *I describe how to create a redundant storage pool* based on a pair of 1 TB external USB disks in section 2 (Storage pools).

=== Network equipment specs


== 2. Hypervisor (Proxmox VE) installation

=== Proxmox version

My first build was based on a standard bare-metal **Ubuntu Server 22.04** installation. This setup was perfectly ok for my needs and worked as expected, but I had to create from the scratch several common solutions (like backups for configuration files, package installation automation, disk SMART alarms monitoring, etc). This type of adhoc solutions are a heavy burden to maintain a system in the long run.

Since I ran this bare-metal Ubuntu setup for almost 9 months, I knew my small minipc was mainly idle for my workloads. At that time I was reading about hardware virtualization and Type 1 hypervisors (VMMs) and I decided to test Proxmox VE (Virtual Environment), recreating the homelab based on Proxmox.

This kind of software offers a **complete HCI (Hyper-converged Infrastructure) solution ready for professional datacenter management**. Proxmox provides a lot of out-of-the-shelf solutions and best practices making most of the hardware and infrastructure chores very comfortable.

After downloaded *Proxmox VE 7.3* ISO file, I used *ventoy* to flash it in a USB stick. I booted minipc from the USB drive and conducted a common installation. With good defaults, installing a Type 1 hypervisor is not harder than installing a general purpose OS.

Since the minipc final location is in a very uncomfortable place (in the top of a bookshelf), I installed Proxmox VE in a more accessible location using my TV screen and ethernet cable during the process. Since that deployment is basically identical to the definitive setup over the bookshelf, configuring Proxmox networking this way was quite simple.

=== OS Disk partitioning

Proxmox graphical installer comes with very good disk management defaults. Proxmox software is installed only in one disk (/dev/sdb), letting the other disks to create storage pools using the web interface after installation. The final layout of the OS disk (/dev/sdb) looks very professional and at the same time, simple to understand:

[source]
----
>> root@proxmox:~$ lsblk
sdb
├─sdb1
├─sdb2                       vfat        FAT32                    /boot/efi
└─sdb3                       LVM2_member LVM2
  ├─pve-swap                 swap
  ├─pve-root                 ext4        1.0      PROXMOX_ROOT    /
  ├─pve-data_tmeta
  │ └─pve-data-tpool
  │   └─pve-data
  └─pve-data_tdata
    └─pve-data-tpool
      └─pve-data
----

Each partition or volume group has a common and clear goal:
[source]
----
- /boot/efi is where OS-independent bootloader is stored (grub2 in my case)
- pve-swap is the lvm volume group where Proxmox VE placed the swap space
- pve-root voluge group is where the root file system of Proxmox is placed.
- pve-data_tmeta (metadata) is a LVM-based thin provisioning volume used to store virtual hard disks
- pve-data_tdata (data) is a LVM-based thin provisioning volume used to store virtual hard disks
----

=== Networking

Using Proxmox graphical interface makes network setup quite easy. It detected my router physical network out of the box and allowed to set up easily a fixed IP address for Proxmox. This IP is visible in all my LAN and is where Proxmox VE GUI can be found.

My minipc has 2 physical network interfaces (ethernet and wireless):

[source]
----
>> root@proxmox:~$ lspci
01:00.0 Ethernet controller: Intel Corporation Ethernet Controller I225-V (rev 01)
02:00.0 Network controller: Intel Corporation Wireless 3165 (rev 79)
----

My minipc is placed in the top of a bookshelf, close to the router. I wired minipc to the router via an ethernet cable. To lower the power consumption and increase security, wireless interface was not enabled.

Proxmox creates a default ** virtual bridge vmbr0** linked with the default ** ethernet physical NIC eno1** in such a manner each NIC of a VM is created directly in the same network range of the physical network of my router. Quite simple setup and very convenient.

Since I relied on my physical router network (192.168.1.0/24), I found no need to create virtual networks in the proxmox hypervisor. Only two physical address are used of my home network:

[source]
----
  - one for proxmox.homelab (192.168.1.4) => Hypervisor bare-metal deployment
  - one for minipc.homelab (192.148.1.2) => Virtual machine created over Proxmox where docker runs
----

**minipc.homelab** VM has one external IP and lots of private IP address (one for each docker container). Most of the software of the homelab is run inside the virtual machine, making backup and maintenance child's play

[source]
----
>> root@proxmox:~$ ip a
...
2: eno1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq master vmbr0 state UP group default qlen 1000
    link/ether 68:1d:ef:28:1d:0e brd ff:ff:ff:ff:ff:ff
    altname enp1s0
...
4: vmbr0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 68:1d:ef:28:1d:0e brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.4/24 scope global vmbr0
       valid_lft forever preferred_lft forever
    inet6 fe80::6a1d:efff:fe28:1d0e/64 scope link
       valid_lft forever preferred_lft forever
...
----

=== Storage pools

This minipc is intended to *host 1 on-site fault-tolerant copy for my family media (photos and videos)* so only one disk is not a valid option.

*NAS appliances are a common IT solution* that provides both large storage capacity and fault-tolerance. Since you have to buy both the chassis and at least 2 disks, NAS appliances are expensive. If you don't mind to pay that cost, I recommend Synology NAS home appliances.

*There are also open-source NAS servers like TrueNAS, OpenMediaVault or Amahi* but any of the runs directly over docker. Using Proxmox I can create a VM to run this NAS servers but underneath hardware is limited, so I decided to explore Proxmox native storage pools instead of a NAS solution.

Proxmox has a built-in set of storage solutions that can fit my storage requirements:

[source]
----
  - Backup space for my virtual hard disks: A Proxmox directory over my old SSD + backup managing utilies from the Proxmos UI (backup schedulling and restore)
  - Thin provisioning for my virtual hard disks: A Proxmox thin-lvm where virtual hard drive are stored for VMs
  - Redundant store for my family media files: A ZFS zpool using 2 different USB external hard drives of 1 TB.
----

ZFS is used underneath by Proxmox to create a virtual device that synchronize automatically operations over both external hard drives. The model and some technical specs of the USB external hard drives used to create a redundant data store:

[source]
----
>> root@proxmox:~$ smartctl -a /dev/sdb
...
Model Family:     Toshiba 2.5" HDD MQ04UBF... (USB 3.0)
Device Model:     TOSHIBA MQ04UBF100
...
----

[source]
----
>> root@proxmox:~$ lsblk
NAME                FSTYPE      LABEL           MOUNTPOINT         SIZE
sda
└─sda1              ext4        PROXMOX_BACKUPS /mnt/pve/backups   119.2G
sdc
├─sdc1              zfs_member  zfs-mirror-hdd                     931.5G
└─sdc9                                                             8M
sdd
├─sdd1              zfs_member  zfs-mirror-hdd                     931.5G
└─sdd9                                                             8M
zd0                 ext4        VM_100_NAS                         500G
----

=== Additional configuration of proxmox

Proxmox runs by default using 8006 port. In order to use more standard ports, an nginx proxy server can be deployed in proxmox server. Follow this tutorial: https://pve.proxmox.com/wiki/Web_Interface_Via_Nginx_Proxy


Since most of the software is going to be installed inside a VM, at the hypervisor level, very few extra packages are required.

The most important thing missing is to set up email relay for automatic alarms. To configure it, just follow Techno Tim's video: https://www.youtube.com/watch?v=85ME8i4Ry6A

An extract of the configuration steps is the following:

[source]
----
>> apt install -y libsasl2-modules mailutils

# Setup credentials in the sasl_passwd file following this format
>> more /etc/postfix/sasl_passwd
smtp.gmail.com email:passwd

# Create a hashed version of the file
>> postmap hash:/etc/postfix/sasl_passwd
>> chmod 600 /etc/postfix/sasl_passwd

# Paste next configuration in /etc/postfix/main.cf file:
realayhost = smtp.gmail.com:587
smtp_use_tls = yes
smtp_sasl_auth_enable = yes
smtp_sasl_security_options =
smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd
smtp_tls_CAfile = /etc/ssl/certs/Entrust_Root_Certification_Authority.pem

# Restart postfix
>> postfix reload
----

=== Software distributed from hardware verdors

Some hardware is distributed directly from vendor's website. The UPS monitor should be installed at hypervisor level, not VM level. In the event of a power outage, the complete server, including both hypervisor and all vms, should be shut down when the battery is running out.

Initially I deployed and configured the UPS monitor in a VM but this deployment wes neither reliable (sometimes it didn't detect changed in UPS) nor safe (it only shut down the vm and not the hypervisor)

[source]
----
- PowerMaster+: UPS monitor from https://www.powermonitor.software/#PowerMasterPlusSoftware (PowerWalker)
----

== 4. Creating a virtual machine on proxmox

In case of creating VMs from a general purpose Ubuntu server, disable systemd-resolved local DNS server. A good practice is to point primary name server to a local DNS server (if existing) and a secondary name server to a well-known DNS server like Google.

=== Cloud init & cloud images

In general creating VMs from an general-purpose ISO image is not the best approach. Cloud images are a much better alternative. 

https://cloud-images.ubuntu.com/minimal/releases/jammy/release-20230209/
https://pve.proxmox.com/wiki/Cloud-Init_Support

[source]
----
# download the "minimal" cloud image
wget https://cloud-images.ubuntu.com/minimal/releases/jammy/release-20230209/ubuntu-22.04-minimal-cloudimg-amd64.img

# create a new VM with VirtIO SCSI controller
qm create 9000 --memory 2048 --net0 virtio,bridge=vmbr0 --scsihw virtio-scsi-pci

# import the downloaded disk to the local-lvm storage, attaching it as a SCSI drive
qm set 9000 --scsi0 local-lvm:0,import-from=/root/ubuntu-22.04-minimal-cloudimg-amd64.img

# configure a CD-ROM drive, which will be used to pass the Cloud-Init data to the VM
qm set 9000 --ide2 local-lvm:cloudinit

# boot directly from the Cloud-Init image
qm set 9000 --boot order=scsi0

# configure a serial console and use it as a display
qm set 9000 --serial0 socket --vga serial0

# convert to template
qm template 9000
----

=== Ubuntu packages

Most of the applications running in the minipc are deployed as docker containers. However, these ubuntu packages are required to be installed using apt

[source]
----
- qemu-guest-agent: Guest agent for better power managent from host
- docker.io: Docker engine
- docker-compose: Multi-container docker applications
- rclone: Off-site backup
- minidlna: Export media content via DLNA to smart TV
- ssmpt: Link mail command line tool to ssmpt allowing security emails reach my personal account
- mutt: Command line email client to easily sending email programaticaly from shell scripts
----

=== Containers

Running containers

[source]
----
  - Pihole
  - Syncthing
  - Portainer
  - Heimdall
  - Uptime-kuma
  - Photoview
  - Mariadb
  - Watchtower
----

In analysis:

[source]
----
  - Traefik
  - Next-cloud
  - Homeassistant
  - Plex / kodi / jellybin / emby
  - freeipa
  - teleport
----

=== Docker-compose

https://github.com/macvaz/homelab/tree/main/src/docker[YAML file]

== 5. VPN and external access

=== Dynamic DNS
  NoIP
=== Blocking direct traffic to Router DNS
  adblocking (pihole)
  Mainly problematic with Android phones
=== Port forwading for VPN and ¿nextcloud?

=== VPN
  wireguard
  laptop scripts
  mobile phones

== 6. Backups

=== Onsite backups
  syncthing + some bash writing on RAID

=== Offsite backups

https://github.com/macvaz/homelab/tree/main/src/backup/backup_last_month_photos.sh[Monthly backup script using rclone]

