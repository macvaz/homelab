:toc:
:icons: font
:source-highlighter: prettify
:project_id: homelab
:tabsize: 2

== 1. Hardware architecture

Due to the lack of free physical space at home, a software-defined IT infrastructure approach like *hyper-converged infrastructure (HCI)* was found ideal. HCI allows virtualizing most traditional hardware equipment using software-defined computing, networking and storage.

Enterprise-level HCI deployments doesn't require specialized hardware appliances for storage - like NAS appliances - that are very common in traditional IT deployments. Both computing and storage infrastructure can be virtualized by software using hypervisors. Since *Proxmox Virtual Environment (VE) is the most common open-source Type 1 hypervisor* available, it was selected as the base technology of the HCI server.

image::img/physical_architecture.png[]

=== HCI node specs

Hyper-converged infrastructure (HCI) is a software-defined IT infrastructure where storage and computing resources are defined by software (virtualized). Consequently no specialized hardware appliances are required for computation and storage.

==== Nodes

|===
|Node name | Service IP| Operating System| CPUs | Cores| RAM | SSD

|proxmox1
|192.168.1.6
|Proxmox VE 7.36
|1 Intel Celeron (J5040)
|4
|16 GBs
|-1x 0.5TB (OS disk) +
 -2x 1TB (DATA disks)
|===

The smaller SSD disk (OS disk) is for installing Proxmox VE. The other 2 SDD disks (DATA disks) are set up in a RAID 1 deployment to provide fault-tolerant long-term storage (check https://github.com/macvaz/homelab#data-disks-layout[Data disks layout]).

=== KVM access

Since *proxmox1 node* is placed at the top of a bookshelf, there is no easy access when SSH is not available. For core admin chores like UEFI changes or boot failures, the KVM extender is really helpful.

Just connect a USB keyboard and mouse to the receiver KVM (Keyboard Video Mouse) device. There is no additional ethernet cable for the KVM so the ethernet cable linking TV and router is reused to connect KVM receiver and sender. TV losses ethernet connectivity when using KVM extender.

=== Emergency power supply

Uninterrupted power supply (UPS) specs

Describe PowerWalker VI 1000 STL
USB cable
Automatic shutdown

Some hardware is distributed directly from vendor's website. The UPS monitor should be installed at hypervisor level, not VM level. In the event of a power outage, the complete server, including both hypervisor and all vms, should be shut down when the battery is running out.

Initially I deployed and configured the UPS monitor in a VM but this deployment wes neither reliable (sometimes it didn't detect changed in UPS) nor safe (it only shut down the vm and not the hypervisor)

== 2. Installing HCI node (proxmox1)

Proxmox VE offers a **complete HCI (Hyper-converged Infrastructure) solution ready for professional datacenter management**. Proxmox provides a lot of out-of-the-shelf solutions and best practices making most of the hardware and infrastructure chores very comfortable (like automatic backups, VM migrations, etc).

After downloading *Proxmox VE 7.3* ISO file, I used *ventoy* to flash it in a USB stick. I booted proxmox node from the USB drive and conducted a common installation. With good defaults, installing a Type 1 hypervisor is not harder than installing a general purpose OS.

If there is a KVM available, the recommended way of installing Proxmox VE is through the KVM extender. The main benefit is to conduct the OS installation with the server placed in its definitive location with final network connectivity.

=== OS disk layout

Proxmox graphical installer comes with very good disk management defaults. Proxmox software is installed only in one disk (/dev/sdb), letting the other disks for data storage. The final layout of the OS disk (/dev/sdb) looks very professional and at the same time, simple to understand:

|===
|Partition |Volume group |Type| Goal

|sdb1
|-
|
|

|sdb2
|-
|vfat
|/boot/efi is where OS-independent bootloader is stored (grub2 in my case)

|sdb3
|pve-swap
|swap
|lvm volume group where Proxmox VE placed the swap space

|sdb3
|pve-root
|ext4
|lvm volume group where the root file system (/) of Proxmox is placed

|sdb3
|pve-data_tmeta
|lvm thing provisioning
|lvm-based thin provisioning volume used to store (metadata) of virtual hard disks of VMs

|sdb3
|pve-data_tdata
|lvm thing provisioning
|lvm-based thin provisioning volume used to store (data) of virtual hard disks of VMs
|===

[source]
----
>> root@proxmox:~$ lsblk
sdb
├─sdb1
├─sdb2                       vfat        FAT32                    /boot/efi
└─sdb3                       LVM2_member LVM2
  ├─pve-swap                 swap
  ├─pve-root                 ext4        1.0      PROXMOX_ROOT    /
  ├─pve-data_tmeta
  │ └─pve-data-tpool
  │   └─pve-data
  └─pve-data_tdata
    └─pve-data-tpool
      └─pve-data
----

=== Data disks layout

The objective of data disks is to provide a fault-tolerant long-term storage solution for the homelab. Several storage solutions were considered when designing the storage system.

Proxmox supports https://pve.proxmox.com/wiki/Hyper-converged_Infrastructure[2 different HCI storage technologies]:

|===
|Technology |Description | Comments

|Ceph
|A both self-healing and self-managing shared, reliable and highly scalable storage system
|Cluster technology. Thought for having several nodes. Extra administration complexity. Not an appealing option.

|ZFS
|A combined file system and logical volume manager with extensive protection against data corruption, various RAID modes, fast and cheap snapshots
|Memory intensive. Recommended ECC memory. Not really an option

|===

Eventually, both HCI storage technologies were discarded and started to explore approaches similar to *traditional NAS appliances*. NAS servers are a very common IT solution that provides both large storage capacity and fault-tolerance. However, the lack of free space at home, makes having a dedicated hardware NAS appliance not a valid option.

The final approach was to *create a VM (nas_vm), in proxmox1 node, based on the open-source NAS server OpenMediaVault (OMV)*. Proxmox VE allows to create a VM with direct access to both data disks using https://pve.proxmox.com/wiki/Passthrough_Physical_Disk_to_Virtual_Machine_(VM)[disk passthrough]. OpenMediaVault VM (nas_vm) detects both data disks as attached SATA disks, making very easy to create a RAID 1 device over them.

*All storage-related tasks are centralized in the OMV VM (nas_vm)*: managing disks, creating file systems, administering RAID devices, creating SMB shares, creating users, creating and enforcing access policies, controlling quotas, etc. The only data management task done by Proxmox VE is running SMART checks in data disks and sending alarms in the event of failure.

=== Network topology

Using Proxmox graphical interface makes network setup quite easy. It detected my router physical network out of the box and allowed to set up easily a fixed IP address for proxmox1 (192.168.1.6).

Proxmox creates by default ** https://pve.proxmox.com/wiki/Network_Configuration[a virtual bridge (vmbr0)]** linked to the first ** ethernet physical NIC eno1**. This network mode is quite simple, each NIC of a VM connected to this bridge, *gets an IP address directly from my router address space (192.168.1.0/24)*. This network setup is very convenient in a homelab (each VM gets a physical IP address from the router), but it assigns several IP address to the same NIC (same MAC). This setup is normally not valid using proxmox in Cloud Service Providers (CSPs), marking different IPs with same MAC as suspicious devices.

Proxmox VE allows to create additional virtual networks (based on bridged, routed or NATed configurations). No extra virtual networks were created or used. All VMs created in proxmox1 node have only 1 vNIC.

The network diagram of the proxmox1 node (without VMs and docker containers) is the following:

Describe Mikrotik HAP ax2
FW
DNS pointing to pihole

== 3. Installing virtual machines (VMs)

|===
|VM name | Service IP| Operating System| vCPUs (Cores)| RAM | Disks

|docker_vm
|192.168.1.2
|Ubuntu Server 22.04
|3
|3 GBs
|

|nas_vm
|192.168.1.5
|Debian 11
|2
|2 GBs
|

|===

In case of creating VMs from a general purpose Ubuntu server, disable systemd-resolved local DNS server. A good practice is to point primary name server to a local DNS server (if existing) and a secondary name server to a well-known DNS server like Google.



=== Ubuntu packages

Most of the applications running in the minipc are deployed as docker containers. However, these ubuntu packages are required to be installed using apt

[source]
----
- qemu-guest-agent: Guest agent for better power managent from host
- docker.io: Docker engine
- docker-compose: Multi-container docker applications
- rclone: Off-site backup
- minidlna: Export media content via DLNA to smart TV
- ssmpt: Link mail command line tool to ssmpt allowing security emails reach my personal account
- mutt: Command line email client to easily sending email programaticaly from shell scripts
- ddclient: Register dynamic IP in cloudflare
----

=== Containers

Running containers

[source]
----
  - Pihole
  - Syncthing
  - Portainer
  - Heimdall
  - Uptime-kuma
  - Photoview
  - Mariadb
  - Watchtower
----

In analysis:

[source]
----
  - Traefik
  - Next-cloud
  - Homeassistant
  - Plex / kodi / jellybin / emby
  - freeipa
  - teleport
----

=== Docker-compose

https://github.com/macvaz/homelab/tree/main/src/docker[YAML file]

== 5. VPN and external access

=== Dynamic DNS
  NoIP
=== Blocking direct traffic to Router DNS
  adblocking (pihole)
  Mainly problematic with Android phones
=== Port forwading for VPN and ¿nextcloud?

=== VPN
  wireguard
  laptop scripts
  mobile phones

== 6. Backups

=== Onsite backups
  syncthing + some bash writing on RAID

=== Offsite backups

https://github.com/macvaz/homelab/tree/main/src/backup/backup_last_month_photos.sh[Monthly backup script using rclone]

=== Additional configuration of proxmox1

Since most of the software is going to be installed inside a VM, at the hypervisor level, very few extra packages are required.

The most important thing missing is to set up email relay for automatic alarms. To configure it, just follow Techno Tim's video: https://www.youtube.com/watch?v=85ME8i4Ry6A

An extract of the configuration steps is the following:

[source]
----
>> apt install -y libsasl2-modules mailutils

# Setup credentials in the sasl_passwd file following this format
>> more /etc/postfix/sasl_passwd
smtp.gmail.com email:passwd

# Create a hashed version of the file
>> postmap hash:/etc/postfix/sasl_passwd
>> chmod 600 /etc/postfix/sasl_passwd

# Paste next configuration in /etc/postfix/main.cf file:
realayhost = smtp.gmail.com:587
smtp_use_tls = yes
smtp_sasl_auth_enable = yes
smtp_sasl_security_options =
smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd
smtp_tls_CAfile = /etc/ssl/certs/Entrust_Root_Certification_Authority.pem

# Restart postfix
>> postfix reload
----

[source]
----
- PowerMaster+: UPS monitor from https://www.powermonitor.software/#PowerMasterPlusSoftware (PowerWalker)
----

=== Cloud init & cloud images

In general creating VMs from an general-purpose ISO image is not the best approach. Cloud images are a much better alternative.

https://cloud-images.ubuntu.com/minimal/releases/jammy/release-20230209/
https://pve.proxmox.com/wiki/Cloud-Init_Support

[source]
----
# download the "minimal" cloud image
wget https://cloud-images.ubuntu.com/minimal/releases/jammy/release-20230209/ubuntu-22.04-minimal-cloudimg-amd64.img

# create a new VM with VirtIO SCSI controller
qm create 9000 --memory 2048 --net0 virtio,bridge=vmbr0 --scsihw virtio-scsi-pci

# import the downloaded disk to the local-lvm storage, attaching it as a SCSI drive
qm set 9000 --scsi0 local-lvm:0,import-from=/root/ubuntu-22.04-minimal-cloudimg-amd64.img

# configure a CD-ROM drive, which will be used to pass the Cloud-Init data to the VM
qm set 9000 --ide2 local-lvm:cloudinit

# boot directly from the Cloud-Init image
qm set 9000 --boot order=scsi0

# configure a serial console and use it as a display
qm set 9000 --serial0 socket --vga serial0

# convert to template
qm template 9000
----

