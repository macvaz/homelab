:toc:
:icons: font
:source-highlighter: prettify
:project_id: homelab
:tabsize: 2

== 1. Hardware architecture

Due to the lack of free physical space at home, using software-defined IT infrastructure like *hyper-converged infrastructure (HCI)* was ideal. HCI allows virtualizing most traditional hardware equipment (mainly computing and storage) on general-purpose unified hardware platform, without the need of specialized appliances.

Enterprise-level HCI deployments doesn't require specialized hardware appliances for storage - like NAS appliances - that are very common in traditional IT deployments. Both computing and storage infrastructure can be virtualized by software using hypervisors. Since *https://www.proxmox.com/en/proxmox-ve[Proxmox Virtual Environment (VE)] is the most common open-source Type 1 hypervisor* available, it was selected as the base technology of the HCI server.

https://www.proxmox.com/en/proxmox-ve[Proxmox VE] offers a **complete HCI (Hyper-converged Infrastructure) solution ready for professional datacenter management**. Proxmox provides a lot of out-of-the-shelf solutions and best practices making most of the hardware and infrastructure chores very comfortable (like automatic backups, VM migrations, etc).

image::img/physical_architecture.png[]

=== HCI node specs

Hyper-converged infrastructure (HCI) is a software-defined IT infrastructure where storage and computing resources are defined by software (virtualized) on aa unified hardware platform. Consequently no specialized hardware appliances are required for computation and storage.

==== Host OS (proxmox nodes)

Currently, there is only one hyperconverged node in the deployment: proxmox1. It's hardware specs are the following:

|===
|Host OS name | Service IP| Operating System| CPUs | Cores| RAM | SSD

|proxmox1
|192.168.1.6
|Proxmox VE 7.36
|1 Intel Celeron (J5040)
|4
|16 GBs
|-1x 0.5TB (OS disk) +
 -2x 1TB (DATA disks)
|===

*Low power consumption was a design principle* used during hardware selection. Since CPU performance requirements for a domestic server are quite easy to fulfill, even with low-end processors like Intel Celeron family, CPU model and motherboard were specifically selected trying to maximize the ratio of performance / power. With a TDP (thermal design power) of 10 watts, both CPU model (Intel Celeron J5040) and mother-board (Asrock J5040 ITX) were outstanding options.

All disks are SSD, which not only are very performant with very low latency times but also power efficient (less than 1 watt of power consumption per disk). The smallest SSD disk (OS disk) is for installing Proxmox VE hypervisor. The other 2 SDD disks (DATA disks) are set up in a RAID 1 deployment to provide fault-tolerant long-term storage (check https://github.com/macvaz/homelab#data-disks-layout[Data disks layout]).

=== Emergency power supply

One of the most important goals of the homelab is to remove our dependency with external cloud storage providers (like Google Drive), creating a fault-tolerant long-term private storage for domestic use. Consequently, having a reliable power supply to our storage server is key to guarantee a safe long-term data archival of my family media and sensitive documents.

*PowerWalker VI 1000 STL is a monitorized UPS (uninterrupted power supply)* that ensures power supply in event of power outage for nearly 1 hour to the router and the proxmox1 node. If the external power supply has not been restored during an hour, the UPS starts a graceful shutdown of the node via the monitoring USB cable that connects the UPS and proxmox1 node. Having a very power efficient hardware (around 20 watts) is key to have a emergency power supply of almost an hour.

In order to control the graceful shutdown, *a monitoring agent has to be installed in the proxmox1 HCI node*. There were to possibilities for this installation: installing in the host OS (proxmox1) or in a guest OS (VM). Proxmox VE is able to easily virtualize a USB device connected to the host OS, making it available for the guest OS. However, the connection of the UPS with the virtualized monitor agent was neither reliable (sometimes it didn't detect changes in UPS status) nor safe (it only shutted down the guest OS and not the complete node).

After installing the UPS monitor agent in the host OS (proxmox), all this unreliable behaviour stopped. *UPS monitor is the only software installed directly in the hypervisor (host OS)*, without creating any VM or LXC container. The monitoring agent is distributed directly from https://www.powermonitor.software/#PowerMasterPlusSoftware[vendor's website].

== 2. Host operating system (Proxmox hypervisor)

=== KVM access

Since *proxmox1 node* is placed at the top of a bookshelf, there is no easy access when SSH is not available. For core admin chores like UEFI changes, host OS installation or debugging boot failures, a KVM extender is really handy.

Just connect a USB keyboard and mouse to the receiver KVM (Keyboard Video Mouse) device. Since there is no additional ethernet cable for the KVM extender, the ethernet cable linking TV and router is reused when utilizing the KVM. TV losses ethernet connectivity when using KVM extender.

Using a KVM extender during host OS installation has a main benefit: installing with the server places in its definitive location with final network connectivity.

=== Installation

After downloading *Proxmox VE 7.3* ISO file, I used *ventoy* to flash it in a USB stick. I booted proxmox node from the USB drive and conducted a common installation. With good defaults, installing a Type 1 hypervisor is not harder than installing a general purpose OS.

=== OS disk layout

Proxmox graphical installer comes with very good disk management defaults. *Proxmox software is installed only in the OS disk (/dev/sdb), letting the other disks for data storage*. The final layout of the OS disk (/dev/sdb) looks very professional and at the same time, simple to understand:

|===
|Partition |LVM|Type| Goal

|sdb1
|-
|ext2?
|Grub2 OS-independent bootloader partition

|sdb2
|-
|vfat
|EFI System Partition (ESP), which makes it possible to boot on EFI systems. Linux kernel images are stored in this partition and mounted in /boot/efi

|sdb3
|-PV: pve -VG: pve +
-LV: swap
|swap
|lvm LV where Proxmox VE places the swap space

|sdb3
|-PV: pve -VG: pve +
-LV: root
|ext4
|lvm LG mounted as the root file system (/) of Proxmox

|sdb3
|-PV: pve -VG: pve +
-LV: data
|LVM-thin
|lvm thin provisioning volume used to store vDisks

|===

=== Data disks layout

The objective of data disks is to provide a fault-tolerant long-term storage solution for the homelab. Several storage solutions were considered when designing the storage system.

Proxmox supports https://pve.proxmox.com/wiki/Hyper-converged_Infrastructure[2 different HCI storage technologies]:

|===
|Technology |Description | Comments

|Ceph
|A both self-healing and self-managing shared, reliable and highly scalable storage system
|Cluster technology. Thought for having several nodes. Extra administration complexity. Not an appealing option.

|ZFS
|A combined file system and logical volume manager with extensive protection against data corruption, various RAID modes, fast and cheap snapshots
|Memory intensive. Recommended ECC memory. Not really an option

|===

Eventually, both HCI storage technologies were discarded and started to explore approaches similar to *traditional NAS appliances*. NAS servers are a very common IT solution that provides both large storage capacity and fault-tolerance. However, the lack of free space at home, makes having a dedicated hardware NAS appliance not a valid option.

The final approach was to *create a VM (nas), in proxmox1 node, based on the open-source NAS server https://www.openmediavault.org/[OpenMediaVault (OMV)]*. Proxmox VE allows to create a VM with direct access to both data disks using https://pve.proxmox.com/wiki/Passthrough_Physical_Disk_to_Virtual_Machine_(VM)[disk passthrough]. *OpenMediaVault VM (nas) detects both data disks as attached SATA disks*, making very easy to create a RAID 1 device over them.

*All storage-related tasks are centralized in the OpenMediaVault*: managing disks, creating file systems, administering RAID devices, creating SMB shares, creating users, creating and enforcing access policies, controlling quotas, etc. The only data management task done by Proxmox VE is running SMART checks in data disks and sending alarms in the event of failure.

=== Network topology

Using Proxmox graphical interface makes networking setup quite easy. It detected my home physical network (192.168.1.0/24) out of the box and allowed to set up easily a fixed IP address for proxmox1 (192.168.1.6).

The final deployment consists in 2 ip networks:


|===
|Network address |Visibility|Virtualization technology|Connected devices

|192.168.1.0/24
|External
|Physical + virtual switch (vmbr0) in Proxmox VE
|Physical devices and VM vNICs

|10.10.10.0/24
|Internal to docker VM
|Virtual devices (docker0) in Docker
|Docker containers

|===


Proxmox creates by default ** https://pve.proxmox.com/wiki/Network_Configuration[a virtual bridge (vmbr0)]** in proxmox1 node. *This bridge works as a switch, effectively extending my home physical network (192.168.1.0/24) to any VM created inside proxmox1 node*. This bridged network setup is very convenient in a homelab environment (each VM gets a physical IP address from the router, making all VMs available from all devices), but it assigns several IP addresses to the same physical NIC (proxmox's eno1). This setup is normally not allowed in CSPs (Cloud Service Providers), where networking equipment (CSP routers and switches) block traffic coming from different VMs with the same MAC address.

Apart from virtual networking devices created by Proxmox VE, there also another networking virtualization technology: Docker. *Internal to docker VM, exists a software-defined  network (10.10.10.0/24), only used by docker containers to communicate each other*. Physical devices (like mikrotik router, TV and mobile clients) are totally unaware of this internal network, that is not addressable from them.

Proxmox VE allows to create additional virtual networks (based on bridged, routed or NATed configurations). No extra virtual networks were created or used. All VMs created in proxmox1 node have only 1 vNIC.

The network diagram of the proxmox1 node (without docker containers) is the following:

image::img/network_diagram.png[]

== 3. Guest operating systems (Proxmox VMs)

After describing the hardware architecture (host OS and physical devices) in chapter 1 and 2, this chapter describes the software-defined infrastructure (VMs and virtual networks) and the logical architecture of the software deployed on the VMs.

There are 2 VMs with very different responsibilities:


|===
|VM name |Type |Goal

|docker
|Computation
|VM where all docker containers are executed. Uses shared storage drives served by nas VM.

|nas
|Storage
|Centralizes all shared storage devices, technologies and services (RAID 1, SMB drives, access control). Based on open-source NAS server OpenMediaVault (OMV)
|===

A more detailed description of the hardware specs of the guest VMs is listed here:

|===
|Guest OS name | Type | Service IP| Operating System| vCPUs (Cores)| RAM | vDisks

|docker
|Proxmox VM
|192.168.1.2
|Ubuntu Server 22.04
|3
|3 GBs
|2 vDisks

|nas
|Proxmox VM
|192.168.1.5
|Debian 11
|2
|2 GBs
|1 vDisk + 2 SDD disks (via disk passthrough)

|===

In case of creating VMs from a general purpose Ubuntu server, disable systemd-resolved local DNS server. A good practice is to point primary name server to a local DNS server (if existing) and a secondary name server to a well-known DNS server like Google.

=== docker VM

Most of the applications running in the minipc are deployed as docker containers. However, these ubuntu packages are required to be installed using apt

[source]
----
- qemu-guest-agent: Guest agent for better power managent from host
- docker.io: Docker engine
- docker-compose: Multi-container docker applications
- rclone: Off-site backup
- minidlna: Export media content via DLNA to smart TV
- ssmpt: Link mail command line tool to ssmpt allowing security emails reach my personal account
- mutt: Command line email client to easily sending email programaticaly from shell scripts
- ddclient: Register dynamic IP in cloudflare
----

==== Containers

Running containers

[source]
----
  - Pihole
  - Syncthing
  - Portainer
  - Heimdall
  - Uptime-kuma
  - Photoview
  - Mariadb
  - Watchtower
----

In analysis:

[source]
----
  - Traefik
  - Next-cloud
  - Homeassistant
  - Plex / kodi / jellybin / emby
  - freeipa
  - teleport
----

==== Docker-compose

https://github.com/macvaz/homelab/tree/main/src/docker[YAML file]

=== nas VM

Description of open media vault installatioin and setup.



