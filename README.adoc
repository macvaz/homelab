:toc:
:icons: font
:source-highlighter: prettify
:project_id: homelab
:tabsize: 2

== 1. Hardware architecture

Due to the lack of free physical space at home, a software-defined IT infrastructure approach like *hyper-converged infrastructure (HCI)* was found ideal. HCI allows virtualizing most traditional hardware equipment using software-defined computing, networking and storage.

Enterprise-level HCI deployments doesn't require specialized hardware appliances for storage - like NAS appliances - that are very common in traditional IT deployments. Both computing and storage infrastructure can be virtualized by software using hypervisors. Since *https://www.proxmox.com/en/proxmox-ve[Proxmox Virtual Environment (VE)] is the most common open-source Type 1 hypervisor* available, it was selected as the base technology of the HCI server.

image::img/physical_architecture.png[]

=== HCI node specs

Hyper-converged infrastructure (HCI) is a software-defined IT infrastructure where storage and computing resources are defined by software (virtualized). Consequently no specialized hardware appliances are required for computation and storage.

==== Host OS (proxmox nodes)

|===
|Host OS name | Service IP| Operating System| CPUs | Cores| RAM | SSD

|proxmox1
|192.168.1.6
|Proxmox VE 7.36
|1 Intel Celeron (J5040)
|4
|16 GBs
|-1x 0.5TB (OS disk) +
 -2x 1TB (DATA disks)
|===

The smaller SSD disk (OS disk) is for installing Proxmox VE. The other 2 SDD disks (DATA disks) are set up in a RAID 1 deployment to provide fault-tolerant long-term storage (check https://github.com/macvaz/homelab#data-disks-layout[Data disks layout]).

=== KVM access

Since *proxmox1 node* is placed at the top of a bookshelf, there is no easy access when SSH is not available. For core admin chores like UEFI changes or boot failures, the KVM extender is really helpful.

Just connect a USB keyboard and mouse to the receiver KVM (Keyboard Video Mouse) device. Since there is no additional ethernet cable for the KVM extender, the ethernet cable linking TV and router is reused when utilizing the KVM. TV losses ethernet connectivity when using KVM extender.

=== Emergency power supply

One of the most important goals of the homelab is to remove our dependency with external cloud storage providers (like Google Drive), creating a fault-tolerant long-term private storage for domestic use. Consequently, having a reliable power supply to our storage server is key to guarantee a safe long-term data archival of my family media and sensitive documents.

*PowerWalker VI 1000 STL is a monitorized UPS (uninterrupted power supply)* that ensures power supply in event of power outage  for near 1 hour to the router and the proxmox1 node. If the external power supply has not been restored during that hour, the UPS starts a graceful shutdown of the node via the monitoring USB cable that connects the UPS and proxmox1 node.

In order to control the graceful shutdown, *a monitoring agent has to be installed in the proxmox1 HCI node*. There were to possibilities for this installation: in the host OS (proxmox1) or in  guest OS (VM). Proxmox VE is able to easily virtualize a USB device connected to the host OS, making it available for the guest OS. However, the connection of the UPS and the monitor running in a VM was neither reliable (sometimes it didn't detect changes in UPS status) nor safe (it only shutted down the guest OS and not the complete node).

After installing the UPS monitor agent in the host OS (proxmox), all this unreliable behaviour stopped. *UPS monitor is the only software installed directly in the hypervisor (host OS)*, without creating any VM or LXC container. The monitoring agent is distributed directly from vendor's website:

[source]
----
- PowerMaster+: UPS monitor from https://www.powermonitor.software/#PowerMasterPlusSoftware (PowerWalker)
----

== 2. Host operating system (Proxmox hypervisor)

https://www.proxmox.com/en/proxmox-ve[Proxmox VE] offers a **complete HCI (Hyper-converged Infrastructure) solution ready for professional datacenter management**. Proxmox provides a lot of out-of-the-shelf solutions and best practices making most of the hardware and infrastructure chores very comfortable (like automatic backups, VM migrations, etc).

After downloading *Proxmox VE 7.3* ISO file, I used *ventoy* to flash it in a USB stick. I booted proxmox node from the USB drive and conducted a common installation. With good defaults, installing a Type 1 hypervisor is not harder than installing a general purpose OS.

If there is a KVM (Keyboard Video Mouse) available, the recommended way of installing Proxmox VE is through the KVM extender. The main benefit is to conduct the OS installation with the server placed in its definitive location with final network connectivity.

=== OS disk layout

Proxmox graphical installer comes with very good disk management defaults. P*roxmox software is installed only in one disk (/dev/sdb), letting the other disks for data storage*. The final layout of the OS disk (/dev/sdb) looks very professional and at the same time, simple to understand:

|===
|Partition |Volume group |Type| Goal

|sdb1
|-
|
|

|sdb2
|-
|vfat
|/boot/efi is where OS-independent bootloader is stored (grub2 in my case)

|sdb3
|pve-swap
|swap
|lvm volume group where Proxmox VE placed the swap space

|sdb3
|pve-root
|ext4
|lvm volume group where the root file system (/) of Proxmox is placed

|sdb3
|pve-data_tmeta
|lvm thing provisioning
|lvm-based thin provisioning volume used to store (metadata) of virtual hard disks of VMs

|sdb3
|pve-data_tdata
|lvm thing provisioning
|lvm-based thin provisioning volume used to store (data) of virtual hard disks of VMs
|===

[source]
----
>> root@proxmox:~$ lsblk
sdb
├─sdb1
├─sdb2                       vfat        FAT32                    /boot/efi
└─sdb3                       LVM2_member LVM2
  ├─pve-swap                 swap
  ├─pve-root                 ext4        1.0      PROXMOX_ROOT    /
  ├─pve-data_tmeta
  │ └─pve-data-tpool
  │   └─pve-data
  └─pve-data_tdata
    └─pve-data-tpool
      └─pve-data
----

=== Data disks layout

The objective of data disks is to provide a fault-tolerant long-term storage solution for the homelab. Several storage solutions were considered when designing the storage system.

Proxmox supports https://pve.proxmox.com/wiki/Hyper-converged_Infrastructure[2 different HCI storage technologies]:

|===
|Technology |Description | Comments

|Ceph
|A both self-healing and self-managing shared, reliable and highly scalable storage system
|Cluster technology. Thought for having several nodes. Extra administration complexity. Not an appealing option.

|ZFS
|A combined file system and logical volume manager with extensive protection against data corruption, various RAID modes, fast and cheap snapshots
|Memory intensive. Recommended ECC memory. Not really an option

|===

Eventually, both HCI storage technologies were discarded and started to explore approaches similar to *traditional NAS appliances*. NAS servers are a very common IT solution that provides both large storage capacity and fault-tolerance. However, the lack of free space at home, makes having a dedicated hardware NAS appliance not a valid option.

 The final approach was to *create a VM (nas), in proxmox1 node, based on the open-source NAS server https://www.openmediavault.org/[OpenMediaVault (OMV)]*. Proxmox VE allows to create a VM with direct access to both data disks using https://pve.proxmox.com/wiki/Passthrough_Physical_Disk_to_Virtual_Machine_(VM)[disk passthrough]. *OpenMediaVault VM (nas) detects both data disks as attached SATA disks, making very easy to create a RAID 1 device over them*.

*All storage-related tasks are centralized in the OMV VM (nas_vm)*: managing disks, creating file systems, administering RAID devices, creating SMB shares, creating users, creating and enforcing access policies, controlling quotas, etc. The only data management task done by Proxmox VE is running SMART checks in data disks and sending alarms in the event of failure.

=== Network topology

Using Proxmox graphical interface makes networking setup quite easy. It detected my home physical network (192.168.1.0/24) out of the box and allowed to set up easily a fixed IP address for proxmox1 (192.168.1.6).

The final deployment consists in 2 ip networks:


|===
|Network address |Visibility|Virtualization technology|Connected devices

|192.168.1.0/24
|External
|Physical + virtual switch (vmbr0) in Proxmox VE
|Physical devices and VM vNICs

|10.10.10.0/24
|Internal
|Virtual devices (docker0) in Docker
|Docker containers

|===


Proxmox creates by default ** https://pve.proxmox.com/wiki/Network_Configuration[a virtual bridge (vmbr0)]** in proxmox1 node. *This bridge works as a switch, effectively extending my home physical network (192.168.1.0/24) to any VM created inside proxmox1 node*. This bridged network setup is very convenient in a homelab environment (each VM gets a physical IP address from the router), but it assigns several IP addresses to the same physical NIC (proxmox's eno1). This setup is normally not allowed in Cloud Service Providers (CSPs), where networking equipment (CSP routers and switches) block traffic coming from different VMs with the same MAC address.

Apart from virtual networking devices created by Proxmox VE, there also another networking virtualization technology (Docker) internal the docker VM (192.168.1.2/24). *This software-defined Docker network (10.10.10.0/24) is internal* and never leaves docker VM, so all physical devices (mikrotik router, TV and mobile clients) are totally unaware of this internal network. This network is only used by docker containers to communicate each other.

Proxmox VE allows to create additional virtual networks (based on bridged, routed or NATed configurations). No extra virtual networks were created or used. All VMs created in proxmox1 node have only 1 vNIC.

The network diagram of the proxmox1 node (without docker containers) is the following:

image::img/network_diagram.png[]

== 3. Guest operating systems (Proxmox VMs)

After describing the hardware architecture (host OS and physical devices) in chapter 1 and 2, this chapter describes the software-defined infrastructure (VMs and virtual networks) and the logical architecture of the software deployed on the VMs.

There are 2 VMs with very different responsibilities:
[source]
----
 - docker: where all docker containers are executed. Uses shared storage drives served by nas VM.
 - nas: centralizes all shared storage technologies and services (RAID 1, SMB drives, access control). Based on open-source NAS server OpenMediaVault (OMV)
----

A more detailed description of the hardware specs of the guest VMs is listed here:

|===
|Guest OS name | Type | Service IP| Operating System| vCPUs (Cores)| RAM | vDisks

|docker
|Proxmox VM
|192.168.1.2
|Ubuntu Server 22.04
|3
|3 GBs
|

|nas
|Proxmox VM
|192.168.1.5
|Debian 11
|2
|2 GBs
|

|===

In case of creating VMs from a general purpose Ubuntu server, disable systemd-resolved local DNS server. A good practice is to point primary name server to a local DNS server (if existing) and a secondary name server to a well-known DNS server like Google.

=== Ubuntu packages

Most of the applications running in the minipc are deployed as docker containers. However, these ubuntu packages are required to be installed using apt

[source]
----
- qemu-guest-agent: Guest agent for better power managent from host
- docker.io: Docker engine
- docker-compose: Multi-container docker applications
- rclone: Off-site backup
- minidlna: Export media content via DLNA to smart TV
- ssmpt: Link mail command line tool to ssmpt allowing security emails reach my personal account
- mutt: Command line email client to easily sending email programaticaly from shell scripts
- ddclient: Register dynamic IP in cloudflare
----

=== Containers

Running containers

[source]
----
  - Pihole
  - Syncthing
  - Portainer
  - Heimdall
  - Uptime-kuma
  - Photoview
  - Mariadb
  - Watchtower
----

In analysis:

[source]
----
  - Traefik
  - Next-cloud
  - Homeassistant
  - Plex / kodi / jellybin / emby
  - freeipa
  - teleport
----

=== Docker-compose

https://github.com/macvaz/homelab/tree/main/src/docker[YAML file]

== 5. VPN and external access

=== Dynamic DNS
  NoIP
=== Blocking direct traffic to Router DNS
  adblocking (pihole)
  Mainly problematic with Android phones
=== Port forwading for VPN and ¿nextcloud?

=== VPN
  wireguard
  laptop scripts
  mobile phones

== 6. Backups

=== Onsite backups
  syncthing + some bash writing on RAID

=== Offsite backups

https://github.com/macvaz/homelab/tree/main/src/backup/backup_last_month_photos.sh[Monthly backup script using rclone]

=== Additional configuration of proxmox1

Since most of the software is going to be installed inside a VM, at the hypervisor level, very few extra packages are required.

The most important thing missing is to set up email relay for automatic alarms. To configure it, just follow Techno Tim's video: https://www.youtube.com/watch?v=85ME8i4Ry6A

An extract of the configuration steps is the following:

[source]
----
>> apt install -y libsasl2-modules mailutils

# Setup credentials in the sasl_passwd file following this format
>> more /etc/postfix/sasl_passwd
smtp.gmail.com email:passwd

# Create a hashed version of the file
>> postmap hash:/etc/postfix/sasl_passwd
>> chmod 600 /etc/postfix/sasl_passwd

# Paste next configuration in /etc/postfix/main.cf file:
realayhost = smtp.gmail.com:587
smtp_use_tls = yes
smtp_sasl_auth_enable = yes
smtp_sasl_security_options =
smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd
smtp_tls_CAfile = /etc/ssl/certs/Entrust_Root_Certification_Authority.pem

# Restart postfix
>> postfix reload
----

=== Cloud init & cloud images

In general creating VMs from an general-purpose ISO image is not the best approach. Cloud images are a much better alternative.

https://cloud-images.ubuntu.com/minimal/releases/jammy/release-20230209/
https://pve.proxmox.com/wiki/Cloud-Init_Support

[source]
----
# download the "minimal" cloud image
wget https://cloud-images.ubuntu.com/minimal/releases/jammy/release-20230209/ubuntu-22.04-minimal-cloudimg-amd64.img

# create a new VM with VirtIO SCSI controller
qm create 9000 --memory 2048 --net0 virtio,bridge=vmbr0 --scsihw virtio-scsi-pci

# import the downloaded disk to the local-lvm storage, attaching it as a SCSI drive
qm set 9000 --scsi0 local-lvm:0,import-from=/root/ubuntu-22.04-minimal-cloudimg-amd64.img

# configure a CD-ROM drive, which will be used to pass the Cloud-Init data to the VM
qm set 9000 --ide2 local-lvm:cloudinit

# boot directly from the Cloud-Init image
qm set 9000 --boot order=scsi0

# configure a serial console and use it as a display
qm set 9000 --serial0 socket --vga serial0

# convert to template
qm template 9000
----

