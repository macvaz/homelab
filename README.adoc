:toc:
:icons: font
:source-highlighter: prettify
:project_id: homelab
:tabsize: 2

== 1. Hardware architecture

In a residential environment, optimizing the number of specilized hardware appliences is key for reducing space. Using modern virtualization servers (hypervisors), make possible running virtual IT infrastructure in an single general-purpose hardware appliance.

https://www.proxmox.com/en/proxmox-ve[Proxmox VE] offers a **complete virtualization solution ready for professional datacenter management**. Proxmox VE is based on KVM kernel virtualization technology and provides a lot of out-of-the-shelf solutions and best practices.

The complete hardware architecture of the homelab is the following:

image::img/physical_architecture.png[]

=== Virtualization node

There is only one virtualization node (proxmox1) that hosts all the virtual infrastructure of the homelab. Its hardware specs are the following:

|===
|Host OS name | IP address| Operating System| CPUs | Cores| RAM | SSD

|proxmox1
|192.168.1.6
|Proxmox VE 7.36 (based on Debian Linux)
|1 Intel Celeron (J5040)
|4
|16 GBs
|-1x 0.5TB (OS disk) +
 -2x 1TB (DATA disks)
|===

Low power consumption was a requirement for hardware selection. *With a TDP (thermal design power) of 15 watts*, both CPU model (Intel Celeron J5040) and mother-board (Asrock J5040 ITX) were outstanding options.

*All disks are SSD (Solid State Disks)*, which not only are very performant with very low latency times but also power efficient (less than 1 watt of power consumption per disk). 

=== Emergency power supply

One of the most important goals of the homelab is to remove our dependency with external cloud storage providers (like Google Drive), creating a fault-tolerant long-term private storage for domestic use. Consequently, *having a reliable power supply to our storage server is key* to guarantee a safe long-term data archival of my family media and sensitive documents.

*PowerWalker VI 1000 STL is a monitorized UPS (uninterrupted power supply)* that ensures power supply in event of power outage for nearly 1 hour to the router and the "proxmox1" node. If the battery is running out of energy, the UPS starts a gracefull shutdown via the USB-C cable that connects the UPS and "proxmox1" node.

=== KVM extender

Since *"proxmox1" node* is placed at the top of a bookshelf, there is no easy access to the machine when SSH is not available. For core admin chores like UEFI changes, host OS installation or debugging boot failures, a KVM extender is really handy.

A *keyboard, video, and mouse (KVM) extender enables users to work locally on a computer from a distance*. KVM extenders overcome the distance limitation of HDMI®, DisplayPort™, and USB cables and transport these signals.

image::img/kvm_extender_diagram.png[]

Some content of this section is taken from https://video.matrox.com/, that provides a great description of what a KVM extender is and how it works.

== 2. Virtualization server (Proxmox hypervisor)

=== Hypervisor installation

Installing *Proxmox Virtual Environment (PVE) 7.3* ISO file, is not harder to install than any Linux-based OS. I used *ventoy* to flash it in a USB stick. I booted "proxmox1" node from the USB drive and conducted a common installation using the KVM extender. 

Using a KVM extender during host OS installation has a main benefit: installing the server OS with all the hardware placed in its definitive location with final network connectivity.

=== OS disk

Proxmox VE graphical installer comes with very good disk management defaults. *Proxmox VE software is installed only in the OS disk (/dev/sdb), letting the other disks for data storage*. The final layout of the OS disk looks very professional and at the same time, simple to understand:

|===
|Partition |LVM LV|Type| Goal

|sdb1
|-
|ext2?
|Grub2 OS-independent bootloader partition

|sdb2
|-
|vfat
|EFI System Partition (ESP), which makes it possible to boot on EFI systems. Linux kernel images are stored in this partition and mounted in /boot/efi

|sdb3
|*swap*
|swap
|lvm LV where Proxmox VE places the swap space

|sdb3
|*root*
|ext4
|lvm LV mounted as the root file system (/) of Proxmox

|sdb3
|*data*
|LVM-thin
|lvm thin provisioning volume used to store vDisks

|===

For clarity, in the above table only LVM logical volumes (LVs) are shown. There is also one physical volume (PV) called "pve" and a volume group (VG) called "pve".

=== Data disks

The objective of data disks is to provide a fault-tolerant long-term storage solution for the homelab. Several storage solutions were considered when designing the storage system.

Proxmox supports https://pve.proxmox.com/wiki/Hyper-converged_Infrastructure[2 different HCI storage technologies]:

|===
|Technology |Description | Comments

|Ceph
|A both self-healing and self-managing shared, reliable and highly scalable storage system
|Cluster technology. Thought for having several nodes. Extra administration complexity. Not an appealing option.

|ZFS
|A combined file system and logical volume manager with extensive protection against data corruption, various RAID modes, fast and cheap snapshots
|Memory intensive. Recommended ECC memory. Not really an option

|===

Since HCI technologies normally relies on creating a physical cluster with several nodes for fault tolerance, both HCI storage technologies were discarded. 
The final approach was to *create a VM "nas", based on the open-source NAS server https://www.openmediavault.org/[OpenMediaVault]* and configuring disk-passthrough from the hypervisor to "nas" vm.

=== Network topology

Using Proxmox graphical interface makes networking setup quite easy. It detected my home physical network (192.168.1.0/24) out of the box and allowed to set up easily a fixed IP address for proxmox1 (192.168.1.6).

The final deployment consists in 2 ip networks:


|===
|Network address |Network type|Virtualization technology|Connected devices

|192.168.1.0/24
|Physical
|Physical network + virtual bridge in proxmox1 node
|Physical devices and VM vNICs

|10.10.10.0/24
|Virtual
|Virtual bridge (docker0) in Docker
|Docker containers

|===


Proxmox creates by default ** https://pve.proxmox.com/wiki/Network_Configuration[a virtual bridge (vmbr0)]** in "proxmox1" node. *This bridge works as a switch, effectively extending my home physical network (192.168.1.0/24) to any VM created inside "proxmox1" node*. This bridged network provides to each virtual server an IP directly from the router address space.

image::img/network_diagram.png[]

== 3. Virtual servers (Proxmox VMs and LXC containers)

This chapter describes the software-defined infrastructure (VMs and virtual networks) created to run the homelab. There are 2 virtual servers (1 VM and 1 Linux Container) with very different responsibilities:

|===
|Virtual server name |Resource type |IP addresses |Goal

|docker
|Computation
|192.168.1.4 192.168.1.7
|Linux Container (LXC) where all docker containers are executed. Uses SMB shared storage drives served by "nas" VM.

|nas
|Storage
|192.168.1.5
|Virtual machine that centralizes all shared storage devices, technologies and services (RAID 1, SMB drives, access control). Based on open-source NAS server OpenMediaVault
|===


A more detailed description of the virtual server's hardware specs is:

|===
|Virtual server name | Type | Guest OS| vCPUs (Cores)| RAM | Storage

|docker
|LXC Container
|Proxmox LXC debian 11 template
|3
|4 GBs
|- 1 vDisks (docker images storage) +
- external SMB drives

|nas
|Virtual Machine
|OpenMediaVault 6.3 (based on Debian 11)
|2
|3 GBs
|- 1 vDisk (for OS) +
- 2 SDD physical disks (via disk passthrough)

|===

=== Logical architecture

In this section, the *main services and batch jobs* deployed on the homelab are presented. This diagram includes software running in both bare-metal infrastructure (hypervisor) and virtual infrastructure ("nas" and "docker" virtual servers).

In addition, the logical architecture diagram also presents the main external services used by the system. Excluding domain registration, all other services are fee of use. The main external services used are:

[source]
----
- NameCheap: Domain register (thehomelab.site)
- CloudFlare: DNS management
- Let's Encrypt: SSL certificates issuance
- Mega.io: off-site backup
----

image::img/logical_architecture.png[]

=== "nas" VM

Proxmox VE allows to create a VM with direct access to both data disks using https://pve.proxmox.com/wiki/Passthrough_Physical_Disk_to_Virtual_Machine_(VM)[disk passthrough]. *OpenMediaVault VM "nas" detects both data disks as attached SATA disks*, making very easy to create a RAID 1 device over them.

*All storage-related tasks are centralized in the OpenMediaVault*: managing disks, creating file systems, administering RAID devices, creating SMB shares, creating users, creating and enforcing access policies, controlling quotas, etc. The only data management task done by Proxmox VE is running SMART checks in data disks and sending alarms in the event of failure.


Description of https://www.openmediavault.org/[OpenMediaVault] installation and setup

[source]
----
  - RAID 1
  - File systems
  - SMB shares
  - quotas
  - user permissions
----

=== "docker" LXC Container

Most of the applications running in the minipc are deployed as docker containers. However, these ubuntu packages are required to be installed using apt

[source]
----
- qemu-guest-agent: Guest agent for better power managent from host
- docker.io: Docker engine
- docker-compose: Multi-container docker applications
- rclone: Off-site backup
- minidlna: Export media content via DLNA to smart TV
- ssmtp: Link mail command line tool to ssmpt allowing security emails reach my personal account
----

==== Containers

Running containers

[source]
----
  - Pihole
  - Portainer
  - Portfolio
  - Heimdall
  - Checkmk
  - Nextcloud
  - Nginx Proxy Manager
  - Transmission
  - photoprism
----

In order to run pihole DNS service on Ubuntu server, disable systemd-resolved local DNS server. A good practice is to point primary name server to a local DNS server (if existing) and a secondary name server to a well-known DNS server like Google (8.8.8.8).

===== PiHole

===== Nextcloud

With Nextcloud Files, users have easy access and can share and collaborate on their files, photos and documents wherever they are. All that without any data leaks to third parties and having full control over their data.

Nextcloud operation documentation can be found xref:nextcloud.adoc[here].

===== Nginx Proxy Manager

===== Containers to analyze

In analysis:

[source]
----
  - Plex / kodi / jellybin / emby
  - freeipa / Authelia / Keycloack / goauthentik.io/
----

==== Docker-compose

https://github.com/macvaz/homelab/blob/main/docker/docker-compose.yaml
[YAML file]





